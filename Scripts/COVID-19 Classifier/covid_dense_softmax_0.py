import pandas as pd 
import cv2                 
import numpy as np         
import os           
import random
import math       
from random import shuffle
from tqdm import tqdm  
import scipy
import skimage
from skimage.transform import resize
import sys
from glob import glob
import matplotlib.pyplot as plt


import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications.densenet import preprocess_input
from tensorflow.keras import Sequential, Model, Input
from tensorflow.keras.models import  model_from_json
from tensorflow.keras.layers import Dense , Activation
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Flatten
from tensorflow.keras.optimizers import SGD , RMSprop, Adam
from tensorflow.keras.layers import Conv2D , BatchNormalization
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras import backend as K
from tensorflow.keras.models import model_from_json, load_model
from tensorflow.keras.layers import Dense , Activation, GRU, TimeDistributed, RepeatVector
from tensorflow.keras.layers import Flatten, AvgPool2D, MaxPool2D
from tensorflow.keras.layers import Conv2D, BatchNormalization, GlobalAvgPool2D
from tensorflow.keras.applications.densenet import DenseNet121, DenseNet169
from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau


os.environ["CUDA_VISIBLE_DEVICES"]="0"

def histogram_equalize(img):
    # convert image from RGB to HSV
    img_hsv = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_RGB2HSV)
    # Histogram equalisation on the V-channel
    img_hsv[:, :, 2] = cv2.equalizeHist(img_hsv[:, :, 2])
    # convert image back from HSV to RGB
    img = cv2.cvtColor(img_hsv, cv2.COLOR_HSV2RGB)
    return img

def RandomResizedCrop(img, size=(224, 224), scale=(0.5, 1.0), ratio=(3./ 4., 4./3.), occlusion=False, interpolation=cv2.INTER_LINEAR):
    
    """Get parameters for ``crop`` for a random sized crop.

    Args:
        img (PIL Image): Image to be cropped.
        scale (tuple): range of size of the origin size cropped
        ratio (tuple): range of aspect ratio of the origin aspect ratio cropped

    Returns:
        tuple: params (i, j, h, w) to be passed to ``crop`` for a random
            sized crop.
    """
    width, height = img.shape[0], img.shape[1]
    area = height * width

    cond = random.random()
    if  cond > 0.5:
        offset = random.randint(height//6, height//4)
        img[:offset] = 0.0

    for _ in range(10):
        target_area = random.uniform(*scale) * area
        log_ratio = (math.log(ratio[0]), math.log(ratio[1]))
        aspect_ratio = math.exp(random.uniform(*log_ratio))

        w = int(round(math.sqrt(target_area * aspect_ratio)))
        h = int(round(math.sqrt(target_area / aspect_ratio)))

        if 0 < w <= width and 0 < h <= height:
            i = random.randint(0, height - h)
            j = random.randint(0, width - w) 
            img = img[i:(i + h), j:(j + w)]
            img = cv2.resize(img, size, interpolation)
            # if np.random.random() > 0.5:
            #     img = np.flipud(img)
            return img

    # Fallback to central crop
    in_ratio = float(width) / float(height)
    if (in_ratio < min(ratio)):
        w = width
        h = int(round(w / min(ratio)))
    elif (in_ratio > max(ratio)):
        h = height
        w = int(round(h * max(ratio)))
    else:  # whole image
        w = width
        h = height
    i = (height - h) // 2
    j = (width - w) // 2
    img = img[i:(i + h), j:(j + w)]
    img = cv2.resize(img, size, interpolation)
    return img      


def crop_generator(batches, target_size=224, scale=(0.4, 1.0), occlusion=False):
    """Take as input a Keras ImageGen (Iterator) and generate random
    crops from the image batches generated by the original iterator.
    """
    while True:
        batch_x, batch_y = next(batches)
        batch_crops = np.zeros((batch_x.shape[0], target_size[0], target_size[1], 3))
        for i in range(batch_x.shape[0]):
            # Random contrast equalization
            # if random.random() > 0.5:
            #     batch_crops[i] = histogram_equalize(batch_crops[i])
            batch_crops[i] = RandomResizedCrop(batch_x[i], size=target_size, scale=scale, occlusion=occlusion)            
        batch_crops = preprocess_input(batch_crops)
        yield (batch_crops, batch_y)


def build_pretrained_model(model_file, isTrain=True):
    with open(model_file + '.json', 'r') as f:
        model = model_from_json(f.read())
    model.load_weights(model_file + '_weights.best.hdf5')
    if isTrain is False:
        model.trainable = False
    return model
    

def weighted_binary_crossentropy(weights, alpha=0.2, n_classes=3):
    """
    A weighted version of keras.objectives.categorical_crossentropy
    Variables:
    weights: numpy array of shape (C,) where C is the number of classes
    
    Usage:
    weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.
    loss = weighted_categorical_crossentropy(weights)
    model.compile(loss=loss,optimizer='adam')
    """
    weights = K.variable(weights)

    def loss(y_true, y_pred):
        # soft labeling
        y_true_soft = (1 - alpha) * y_true + alpha / n_classes
        # clip to prevent NaN's and Inf's
        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())
        # calc
        loss = y_true_soft * K.log(y_pred) * weights + (1-y_true_soft) * K.log(1-y_pred)
        loss = - K.mean(loss, -1)
        return loss
    return loss


def weighted_binary_crossentropy_(weights, alpha=0.2, n_classes=3):
    """
    A weighted version of keras.objectives.categorical_crossentropy
    Variables:
    weights: numpy array of shape (C,) where C is the number of classes
    
    Usage:
    weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.
    loss = weighted_categorical_crossentropy(weights)
    model.compile(loss=loss,optimizer='adam')
    """
    weights = K.variable(weights)

    def loss(y_true, y_pred):
        # soft labeling
        y_true_soft = (1 - alpha) * y_true + alpha / n_classes
        # clip to prevent NaN's and Inf's
        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())
        # calc
        loss = y_true_soft * K.log(y_pred) + (1-y_true_soft) * K.log(1-y_pred)
        loss = - K.mean(loss * weights, -1)
        return loss
    return loss

def get_data(Dir):
    image_path = []
    file_name=[]
    target = []
    type_of_pneumonia = []
    for nextDir in os.listdir(Dir):
        if not nextDir.startswith('.'):
            if nextDir in ['NORMAL']:
                label = 'normal'
            elif nextDir in ['PNEUMONIA']:
                label = 'pneumonia'
#             else:
#                 label = 2                
            temp = Dir + nextDir                
            for file in tqdm(os.listdir(temp)):
                if 'jp' in file:
                    image_path.append(temp + '/' + file)
                    file_name.append(file)
                    target.append(label)
                    if label is 'normal':
                        type_of_pneumonia.append('normal')
                    elif 'bacteria' in file:
                        type_of_pneumonia.append('bacteria')
                    elif 'virus' in file:
                        type_of_pneumonia.append('virus')
    return image_path, file_name, target, type_of_pneumonia

# %% LOAD DATA

root_path = '/scratch/parceirosbr/bigoilict/share/GeoFacies/jose/'
dataset_path = root_path + '.datasets/'
# root_path = '/home/jose/ICA/Covid-19/'
# dataset_path = root_path + 'datasets/'
saving_path = root_path

# %%  ->  LOAD CHEST XRAY DATASET <-

# Define paths
TRAIN_DIR = dataset_path + "chest_xray/chest_xray/train/"
VAL_DIR = dataset_path + "chest_xray/chest_xray/val/"
TEST_DIR =  dataset_path + "chest_xray/chest_xray/test/"

# Getting Data
trn_img_paths, trn_file_name, trn_target, trn_type = get_data(TRAIN_DIR)
val_img_paths, val_file_name, val_target, val_type = get_data(VAL_DIR)
tst_img_paths, tst_file_name, tst_target, tst_type = get_data(TEST_DIR)

# Creating Dataframes
chest_xray_trn_df = pd.DataFrame(trn_img_paths, columns=['Image Path'])
chest_xray_trn_df['filename'] = trn_file_name
chest_xray_trn_df['Target'] = trn_target
chest_xray_trn_df['type_of_pneumonia'] = trn_type
chest_xray_trn_df.dropna(inplace=True)

chest_xray_val_df = pd.DataFrame(val_img_paths, columns=['Image Path'])
chest_xray_val_df['filename'] = val_file_name
chest_xray_val_df['Target'] = val_target
chest_xray_val_df['type_of_pneumonia'] = val_type
chest_xray_val_df.dropna(inplace=True)

chest_xray_tst_df = pd.DataFrame(tst_img_paths, columns=['Image Path'])
chest_xray_tst_df['filename'] = tst_file_name
chest_xray_tst_df['Target'] = tst_target
chest_xray_tst_df['type_of_pneumonia'] = tst_type
chest_xray_tst_df.dropna(inplace=True)


chest_xray_trn_df.type_of_pneumonia = chest_xray_trn_df.type_of_pneumonia.replace({'virus': 'pneumonia',
                                                                                  'bacteria': 'pneumonia'})
chest_xray_val_df.type_of_pneumonia = chest_xray_val_df.type_of_pneumonia.replace({'virus': 'pneumonia',
                                                                                  'bacteria': 'pneumonia'})
chest_xray_tst_df.type_of_pneumonia = chest_xray_tst_df.type_of_pneumonia.replace({'virus': 'pneumonia',
                                                                                  'bacteria': 'pneumonia'})
                                                                                  
# Showing histogram
label_counts = chest_xray_trn_df['type_of_pneumonia'].value_counts()
fig, ax1 = plt.subplots(1,1,figsize = (5, 5))
ax1.bar(np.arange(len(label_counts))+0.5, label_counts)
ax1.set_xticks(np.arange(len(label_counts))+0.5)
_ = ax1.set_xticklabels(label_counts.index, rotation = 0)

# %% ->   LOAD RSNA DATASET   <- 

# Setting path for images
rsna_dataset_path_jpg = dataset_path + 'kaggle-pneumonia-jpg/'
rsna_dataset_imgs = rsna_dataset_path_jpg + 'stage_2_train_images_jpg/'

# TODO
rsna_df = pd.read_csv(root_path + 'rsna_dataset.csv')

# Scanning image folder
image_paths = {os.path.basename(x): x for x in 
                   glob(os.path.join(rsna_dataset_imgs,'*.jpg'))}
print('Scans found:', len(image_paths), ', Total Headers', rsna_df['filename'].value_counts().shape[0])

# Adding path information to dataframe
rsna_df['Image Path'] = (rsna_df['filename']+'.jpg').map(image_paths)
rsna_df = rsna_df[['Image Path', 'filename','Target', 'type_of_pneumonia', 'set']]
rsna_df = rsna_df.dropna()
rsna_df.sample(2)

# Selecting training validation, and testing sets
rsna_train_df = rsna_df[rsna_df.set=='train']
rsna_valid_df = rsna_df[rsna_df.set=='valid']
rsna_test_df = rsna_df[rsna_df.set=='test']

print('train', rsna_train_df.shape[0], 'validation', rsna_valid_df.shape[0], 'testing', rsna_test_df.shape[0])

# Showing histogram
label_counts = rsna_train_df['type_of_pneumonia'].value_counts()
fig, ax1 = plt.subplots(1,1,figsize = (5, 5))
ax1.bar(np.arange(len(label_counts))+0.5, label_counts)
ax1.set_xticks(np.arange(len(label_counts))+0.5)
_ = ax1.set_xticklabels(label_counts.index, rotation = 0)
print('\nrsna training set\n', label_counts)

label_counts = rsna_valid_df['type_of_pneumonia'].value_counts()
fig, ax1 = plt.subplots(1,1,figsize = (5, 5))
ax1.bar(np.arange(len(label_counts))+0.5, label_counts)
ax1.set_xticks(np.arange(len(label_counts))+0.5)
_ = ax1.set_xticklabels(label_counts.index, rotation = 0)
print('\nrsna validation set \n',label_counts)

label_counts = rsna_test_df['type_of_pneumonia'].value_counts()
fig, ax1 = plt.subplots(1,1,figsize = (5, 5))
ax1.bar(np.arange(len(label_counts))+0.5, label_counts)
ax1.set_xticks(np.arange(len(label_counts))+0.5)
_ = ax1.set_xticklabels(label_counts.index, rotation = 0)
print('\nrsna testing set \n',label_counts)

# %% ->   LOAD COVID DATASET  <-
covid_dataset_path = dataset_path + 'covid-chestxray-dataset/'
covid_dataset_imgs = covid_dataset_path + 'images/'

# TODO
covid_df = pd.read_csv(root_path +"cohen_dataset.csv")

image_paths = {os.path.basename(x): x for x in 
                   glob(os.path.join(covid_dataset_imgs,'*.*'))}
print('Scans found:', len(image_paths), ', Total Headers', covid_df['filename'].value_counts().shape[0])

# Adding path information to dataframe
covid_df['Image Path'] = (covid_df['filename']).map(image_paths)
covid_df = covid_df[['Image Path', 'filename', 'Target', 'type_of_pneumonia', 'set']]

# Selecting training validation, and testing sets
covid_train_df = covid_df[covid_df.set=='train']
covid_valid_df = covid_df[covid_df.set=='valid']
covid_test_df = covid_df[covid_df.set=='test']

print('train', covid_train_df.shape[0], 'validation', covid_valid_df.shape[0], 'testing', covid_test_df.shape[0])

# Showing histogram
label_counts = covid_train_df['type_of_pneumonia'].value_counts()
fig, ax1 = plt.subplots(1,1,figsize = (5, 5))
ax1.bar(np.arange(len(label_counts))+0.5, label_counts)
ax1.set_xticks(np.arange(len(label_counts))+0.5)
_ = ax1.set_xticklabels(label_counts.index, rotation = 0)
print('\ncovid training set\n', label_counts)

label_counts = covid_valid_df['type_of_pneumonia'].value_counts()
fig, ax1 = plt.subplots(1,1,figsize = (5, 5))
ax1.bar(np.arange(len(label_counts))+0.5, label_counts)
ax1.set_xticks(np.arange(len(label_counts))+0.5)
_ = ax1.set_xticklabels(label_counts.index, rotation = 0)
print('\ncovid validation set \n',label_counts)

label_counts = covid_test_df['type_of_pneumonia'].value_counts()
fig, ax1 = plt.subplots(1,1,figsize = (5, 5))
ax1.bar(np.arange(len(label_counts))+0.5, label_counts)
ax1.set_xticks(np.arange(len(label_counts))+0.5)
_ = ax1.set_xticklabels(label_counts.index, rotation = 0)
print('\ncovid testing set \n',label_counts)


# OVERSAMPLING TRAINING AND VALIDATION COVID AND PENEUMONIA SAMPLES
covid_train_df = pd.concat(20*[covid_train_df], ignore_index=True)
covid_valid_df = pd.concat(20*[covid_valid_df], ignore_index=True)

# %% ->  LOAD FIGURE 1  COVID DATASET  <-
figure1_dataset_path = dataset_path + 'Figure1-COVID-chestxray-dataset/'
figure1_dataset_imgs = figure1_dataset_path + 'images/'

# TODO
figure1_df = pd.read_csv(root_path +"figure1_dataset.csv")

image_paths = {os.path.splitext(os.path.basename(x))[0]: x for x in 
                                  glob(os.path.join(figure1_dataset_imgs,'*.*'))}

print('Scans found:', len(image_paths), ', Total Headers', figure1_df['filename'].value_counts().shape[0])

# Adding path information to dataframe
figure1_df['Image Path'] = (figure1_df['filename']).map(image_paths)


figure1_df = figure1_df[['Image Path', 'filename','Target', 'type_of_pneumonia', 'set']]

# Selecting training validation, and testing sets
figure1_train_df = figure1_df[figure1_df.set=='train']
figure1_valid_df = figure1_df[figure1_df.set=='valid']
figure1_test_df = figure1_df[figure1_df.set=='test']

print('train', figure1_train_df.shape[0], 'validation', figure1_valid_df.shape[0], 'testing', figure1_test_df.shape[0])

# Showing histograms
label_counts = figure1_train_df['type_of_pneumonia'].value_counts()
fig, ax1 = plt.subplots(1,1,figsize = (5, 5))
ax1.bar(np.arange(len(label_counts))+0.5, label_counts)
ax1.set_xticks(np.arange(len(label_counts))+0.5)
_ = ax1.set_xticklabels(label_counts.index, rotation = 0)
print('\nfigure1 training set\n', label_counts)

label_counts = figure1_valid_df['type_of_pneumonia'].value_counts()
fig, ax1 = plt.subplots(1,1,figsize = (5, 5))
ax1.bar(np.arange(len(label_counts))+0.5, label_counts)
ax1.set_xticks(np.arange(len(label_counts))+0.5)
_ = ax1.set_xticklabels(label_counts.index, rotation = 0)
print('\nfigure1 validation set \n',label_counts)

label_counts = figure1_test_df['type_of_pneumonia'].value_counts()
fig, ax1 = plt.subplots(1,1,figsize = (5, 5))
ax1.bar(np.arange(len(label_counts))+0.5, label_counts)
ax1.set_xticks(np.arange(len(label_counts))+0.5)
_ = ax1.set_xticklabels(label_counts.index, rotation = 0)
print('\nfigure1 testing set \n',label_counts)

# OVERSAMPLING TRAINING AND VALIDATION COVID AND PENEUMONIA SAMPLES
figure1_train_df = pd.concat(20*[figure1_train_df], ignore_index=True)
figure1_valid_df = pd.concat(20*[figure1_valid_df], ignore_index=True)


# %% CONCATENATING COHAN AND FIGURE 1 DATASETS AND SPLIT THEM

covid_train_merged_df = pd.concat([covid_train_df, figure1_train_df], ignore_index=True)
covid_valid_merged_df = pd.concat([covid_valid_df, figure1_valid_df], ignore_index=True)
covid_test_merged_df = pd.concat([covid_test_df, figure1_test_df], ignore_index=True)

print('train', covid_train_merged_df.shape[0],
      'validation', covid_valid_merged_df.shape[0], 
      'testing', covid_test_merged_df.shape[0])

# Showing histograms
label_counts = covid_train_merged_df['type_of_pneumonia'].value_counts()
fig, ax1 = plt.subplots(1,1,figsize = (5, 5))
ax1.bar(np.arange(len(label_counts))+0.5, label_counts)
ax1.set_xticks(np.arange(len(label_counts))+0.5)
_ = ax1.set_xticklabels(label_counts.index, rotation = 0)
print('\ncovid merged training set\n', label_counts)

label_counts = covid_valid_merged_df['type_of_pneumonia'].value_counts()
fig, ax1 = plt.subplots(1,1,figsize = (5, 5))
ax1.bar(np.arange(len(label_counts))+0.5, label_counts)
ax1.set_xticks(np.arange(len(label_counts))+0.5)
_ = ax1.set_xticklabels(label_counts.index, rotation = 0)
print('\ncovid merged validation set \n',label_counts)

label_counts = covid_test_merged_df['type_of_pneumonia'].value_counts()
fig, ax1 = plt.subplots(1,1,figsize = (5, 5))
ax1.bar(np.arange(len(label_counts))+0.5, label_counts)
ax1.set_xticks(np.arange(len(label_counts))+0.5)
_ = ax1.set_xticklabels(label_counts.index, rotation = 0)
print('\ncovid merged testing set \n',label_counts)

# %%   Merging Datasets
train_df = pd.concat([rsna_train_df, covid_train_merged_df, chest_xray_trn_df], ignore_index=True)
valid_df = pd.concat([rsna_valid_df, covid_valid_merged_df, chest_xray_val_df], ignore_index=True)
test_df  = pd.concat([rsna_test_df, covid_test_merged_df, chest_xray_tst_df], ignore_index=True)

# %% Defining labels
train_df['normal'] = train_df['type_of_pneumonia']
train_df['pneumonia'] = train_df['type_of_pneumonia']
train_df['covid'] = train_df['type_of_pneumonia']

valid_df['normal'] = valid_df['type_of_pneumonia']
valid_df['pneumonia'] = valid_df['type_of_pneumonia']
valid_df['covid'] = valid_df['type_of_pneumonia']

test_df['normal'] = test_df['type_of_pneumonia']
test_df['pneumonia'] = test_df['type_of_pneumonia']
test_df['covid'] = test_df['type_of_pneumonia']

train_df['covid'] = train_df['covid'].replace({'COVID-19': 1, 'normal':0, 'pneumonia':0}).astype('float32')
train_df['pneumonia'] = train_df['pneumonia'].replace({'COVID-19': 0, 'normal':0, 'pneumonia':1}).astype('float32')
train_df['normal'] = train_df['normal'].replace({'COVID-19': 0, 'normal':1, 'pneumonia':0}).astype('float32')

valid_df['covid'] = valid_df['covid'].replace({'COVID-19': 1, 'normal':0, 'pneumonia':0}).astype('float32')
valid_df['pneumonia'] = valid_df['pneumonia'].replace({'COVID-19': 0, 'normal':0, 'pneumonia':1}).astype('float32')
valid_df['normal'] = valid_df['normal'].replace({'COVID-19': 0, 'normal':1, 'pneumonia':0}).astype('float32')

test_df['covid'] = test_df['covid'].replace({'COVID-19': 1, 'normal':0, 'pneumonia':0}).astype('float32')
test_df['pneumonia'] = test_df['pneumonia'].replace({'COVID-19': 0, 'normal':0, 'pneumonia':1}).astype('float32')
test_df['normal'] = test_df['normal'].replace({'COVID-19': 0, 'normal':1, 'pneumonia':0}).astype('float32')

# %% to categorical
train_df['class'] = train_df['type_of_pneumonia'].map({'normal': 0, 'pneumonia': 1, 'COVID-19': 2})
valid_df['class'] = valid_df['type_of_pneumonia'].map({'normal': 0, 'pneumonia': 1, 'COVID-19': 2})
test_df['class'] = test_df['type_of_pneumonia'].map({'normal': 0, 'pneumonia': 1, 'COVID-19': 2})

# %% Selecting columns labels
columns = ['normal', 'pneumonia', 'covid']

resampled_train_df = train_df.copy()
# %% BALANCING DATA

# BALANCING VALIDATION SAMPLES
c0_samples_df = valid_df[valid_df['class']==0]
c1_samples_df = valid_df[valid_df['class']==1]
c2_samples_df = valid_df[valid_df['class']==2]
# num_of_val = max(c0_samples_df.shape[0], c1_samples_df.shape[0], c2_samples_df.shape[0])
num_of_val = c1_samples_df.shape[0]
res_c0_samples_df = c0_samples_df.sample(num_of_val, replace=True)
res_c1_samples_df = c1_samples_df.sample(num_of_val, replace=False)
res_c2_samples_df = c2_samples_df.sample(num_of_val, replace=True)
resampled_valid_df = pd.concat([res_c0_samples_df,
                                res_c1_samples_df,
                                res_c2_samples_df]).sample(frac=1).reset_index(drop=True)
                                

label_counts = resampled_valid_df['type_of_pneumonia'].value_counts()
fig, ax1 = plt.subplots(1,1,figsize = (5, 5))
ax1.bar(np.arange(len(label_counts))+0.5, label_counts)
ax1.set_xticks(np.arange(len(label_counts))+0.5)
_ = ax1.set_xticklabels(label_counts.index, rotation = 0)
print(label_counts)

# %% COMPUTING WEIGHTS
label_counts = resampled_train_df['type_of_pneumonia'].value_counts()
occurrences = np.array([label_counts['normal'], label_counts['pneumonia'], label_counts['COVID-19']])

weights =  1 / (occurrences / occurrences.max())

print ('weigths...', weights)

# %% Generator to load Data

# load all data in memory
IMG_SIZE = (224, 224)
CROP_SIZE = (256, 256)

load_datagen = ImageDataGenerator()
load_train_data = load_datagen.flow_from_dataframe(dataframe=resampled_train_df,
                                    x_col='Image Path',
                                    y_col=columns,
                                    drop_duplicates=False,
                                    class_mode="raw",
                                    target_size=CROP_SIZE,
                                    color_mode = 'rgb',
                                    batch_size=len(resampled_train_df))
#                                     batch_size=64)

load_valid_data = load_datagen.flow_from_dataframe(dataframe=resampled_valid_df,
                                    x_col='Image Path',
                                    y_col=columns,
                                    drop_duplicates=False,
                                    class_mode="raw",
                                    target_size=IMG_SIZE,
                                    color_mode = 'rgb',
                                    batch_size=len(resampled_valid_df))
#                                     batch_size=64)

load_test_data = load_datagen.flow_from_dataframe(dataframe=test_df,
                                    x_col='Image Path',
                                    y_col=columns,
                                    drop_duplicates=False,
                                    class_mode="raw",
                                    target_size=IMG_SIZE,
                                    color_mode = 'rgb',
                                    batch_size=len(test_df))
#                                     batch_size=64)

#############################################################################
train_datagen = ImageDataGenerator(samplewise_center=False,
                                   samplewise_std_normalization=False,
                                   brightness_range=[0.90, 1.10],
                                   horizontal_flip = True,
                                   vertical_flip = False,
                                   # height_shift_range= 0.1,
                                   # width_shift_range=0.05,
                                   rotation_range=5, 
                                   # shear_range = 0.1,
                                   fill_mode = "constant",
                                   # zoom_range=[0.8, 1.0]
                                  )
valid_datagen = ImageDataGenerator(samplewise_center=False,
                                   samplewise_std_normalization=False,
                                   brightness_range=[0.95, 1.05],
                                   horizontal_flip = True, 
                                   vertical_flip = False, 
                                   preprocessing_function=preprocess_input,
                                   height_shift_range= 0.05,
                                   width_shift_range=0.01,
                                   rotation_range=5, 
                                   # shear_range = 0.01,
                                   fill_mode = "constant",
                                   # zoom_range=0.05
                                  )
test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)


# Load train data to memory
X_train, y_train = next(load_train_data)
X_valid, y_valid = next(load_valid_data)


train_gen = train_datagen.flow(X_train, y_train,
                              batch_size=16,
                              shuffle=True
                              )

valid_gen = valid_datagen.flow(X_valid, y_valid,
                              batch_size=32,
                              shuffle=False
                              )
train_gen_crops = crop_generator(train_gen, IMG_SIZE, scale=(0.8, 1.0), occlusion=True)
# valid_gen_crops = crop_generator(valid_gen, IMG_SIZE, scale=(0.6, 1.0), occlusion=False)
valid_gen_crops = valid_gen


val_X, val_Y = next(valid_gen_crops)


# %% BUILDING MODEL
weighted_loss = weighted_binary_crossentropy_(weights, alpha=0.1)
# IMG_SIZE = (224, 224)
# base_net = DenseNet121(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3), include_top=False, weights='imagenet')

model_file = 'DenseNet_Pneumonia_Softmax'
base_net = build_pretrained_model(root_path + model_file)

x = base_net.get_layer('pool4_relu').output
# x = base_net.output
x = GlobalAvgPool2D(name='global_avg_pooling')(x)
x = Dense(val_Y.shape[1], activation='softmax', name='prediction_layer')(x)
model = Model(inputs=base_net.input, outputs=x, name='model')

model.compile(loss=weighted_loss,
                  optimizer=Adam(lr=1e-5),
                  metrics=['accuracy', 'categorical_crossentropy'])
print(model.summary())

# %%  Experimtents  
name = 'block4_a1lr5s8b16br90102_0_psoft'
model_name = root_path + 'densenet121_covid19_' + name

model_json = model.to_json()
with open(model_name+'.json', "w") as json_file:
    json_file.write(model_json)

    
monitor='val_categorical_crossentropy'

lr_reduce = ReduceLROnPlateau(monitor=monitor, factor=0.1, min_delta=0.0001, patience=1, verbose=1)
weight_path="{}_weights.best.hdf5".format(model_name)

checkpoint = ModelCheckpoint(weight_path, monitor=monitor, verbose=1, 
                             save_best_only=True, mode='min', save_weights_only=True)

early = EarlyStopping(monitor=monitor,
                      mode="min", 
                      patience=5)
callbacks_list = [checkpoint, early, lr_reduce]

history = model.fit(train_gen_crops,
                    steps_per_epoch=len(resampled_train_df)//train_gen.batch_size,
                    validation_data=valid_gen_crops,
                    validation_steps=resampled_valid_df.shape[0]//valid_gen.batch_size,
                    epochs=100,
                    callbacks=callbacks_list)
# %% Saving Results
# del model
# model = build_pretrained_model(model_name)

# test_X, test_Y = next(load_test_data) # one big batch
# test_X = preprocess_input(test_X)

# preds = model.predict(test_X)

train_df.to_csv(root_path + 'train_df_covid_'+name)
valid_df.to_csv(root_path + 'valid_df_covid_'+name)
test_df.to_csv(root_path + 'test_df_covid_'+name)

################# BUILDING MODEL ##########################
# weighted_loss = weighted_binary_crossentropy(weights)
# # IMG_SIZE = (224, 224)
# base_net = DenseNet121(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3), include_top=False, weights='imagenet')

# for layer in base_net.layers:
#     layer.trainable = False

# blocks = ['conv5_block']
# for layer in base_net.layers:
#     for block in blocks:
#         if block in layer.name:
#             layer.trainable = True


# # base_net = keras.models.load_model(root_path + 'DenseNet121.h5')

# # x = base_net.output
# x = base_net.get_layer('conv5_block16_0_relu').output
# # x = Conv2D(1024, 1, 1, padding="same", name='conv2D')(x)
# x = GlobalAvgPool2D(name='global_avg_pooling')(x)
# x = Dense(val_Y.shape[1], activation='softmax', name='prediction_layer')(x)
# model = Model(inputs=base_net.input, outputs=x, name='model')

# model.compile(loss=weighted_loss,
#                   optimizer=Adam(lr=1e-4),
#                   metrics=['accuracy', 'categorical_crossentropy'])
# print(model.summary())

